
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# import streamlit as st

# st.set_page_config(page_title="ğŸ¤– Chatbot Interface", page_icon="ğŸ¤–")
# st.title("ğŸ¤– Chatbot Interface")
# st.markdown("ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒØ¨ÙŠØ±...")

# # -------------------------
# # Model configuration
# # -------------------------
# # model_id = "hugging-quants/Meta-Llama-3.1-8B-Instruct-BNB-NF4"
# # model_id = "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
# # model_id = "hugging-quants/Meta-Llama-3.1-8B-Instruct-BNB-NF4"

# model_id = "meta-llama/Llama-3.1-8B-Instruct"

# device = "cuda" if torch.cuda.is_available() else "cpu"
# st.text(f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø³ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰: {device}")

# # -------------------------
# # Load model with spinner
# # -------------------------
# with st.spinner("Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ Ù‡Ø°Ø§ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¹Ø¯Ø© Ø¯Ù‚Ø§Ø¦Ù‚..."):
#     tokenizer = AutoTokenizer.from_pretrained(model_id)
#     if tokenizer.pad_token_id is None:
#         tokenizer.pad_token_id = tokenizer.eos_token_id

#     model = AutoModelForCausalLM.from_pretrained(
#         model_id,
#         device_map=None,  # CPU only
#         torch_dtype=torch.float32
#     )
#     model.eval()

#     llama_pipeline = pipeline(
#         "text-generation",
#         model=model,
#         tokenizer=tokenizer,
#         max_new_tokens=200,
#         do_sample=True,
#         temperature=0.7,
#         top_k=50,
#         num_return_sequences=1,
#         eos_token_id=tokenizer.eos_token_id,
#         pad_token_id=tokenizer.pad_token_id
#     )

# st.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ âœ…")

# # -------------------------
# # Session state
# # -------------------------
# if "history" not in st.session_state:
#     st.session_state.history = []

# user_input = st.text_input("Ø£Ù†Øª:")

# if user_input:
#     with st.spinner("Ø§Ù„Ø±ÙˆØ¨ÙˆØª ÙŠÙÙƒØ±..."):
#         response = llama_pipeline(user_input)[0]["generated_text"]
#     st.session_state.history.append(("Ø£Ù†Øª", user_input))
#     st.session_state.history.append(("ğŸ¤– Ø§Ù„Ø±ÙˆØ¨ÙˆØª", response))

# # Display chat history
# for sender, msg in st.session_state.history:
#     st.markdown(f"**{sender}:** {msg}")

#########################################################################
# import streamlit as st
# from transformers import pipeline

# st.set_page_config(page_title="ğŸ¤– Chatbot Interface", page_icon="ğŸ¤–")
# st.title("ğŸ¤– Chatbot Interface")
# st.markdown("ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")

# # -------------------------
# # Model configuration via Hugging Face API
# # -------------------------
# model_id = "meta-llama/Llama-3.1-8B-Instruct"

# # Create a text-generation pipeline using the Hugging Face API
# llama_pipeline = pipeline(
#     "text-generation",
#     model=model_id,        # This will use your logged-in token
#     device_map="auto",     # Use GPU if available
#     max_new_tokens=200,
#     do_sample=True,
#     temperature=0.7,
#     top_k=50,
#     pad_token_id=50256
# )

# # -------------------------
# # Session state
# # -------------------------
# if "history" not in st.session_state:
#     st.session_state.history = []

# user_input = st.text_input("Ø£Ù†Øª:")

# if user_input:
#     with st.spinner("Ø§Ù„Ø±ÙˆØ¨ÙˆØª ÙŠÙÙƒØ±..."):
#         response = llama_pipeline(user_input)[0]["generated_text"]

#     st.session_state.history.append(("Ø£Ù†Øª", user_input))
#     st.session_state.history.append(("ğŸ¤– Ø§Ù„Ø±ÙˆØ¨ÙˆØª", response))

# # Display chat history
# for sender, msg in st.session_state.history:
#     st.markdown(f"**{sender}:** {msg}")
#################################### to use API unstead of locally
# from huggingface_hub import InferenceClient
# HF_TOKEN="hf_agav"
# MODEL_ID="meta-llama/Llama-3.1-8B-Instruct"
# client = InferenceClient(model=MODEL_ID, token=HF_TOKEN)
#########################################




import streamlit as st
from huggingface_hub import InferenceClient
import os
from dotenv import load_dotenv

# -------------------------
# Hugging Face API config
# -------------------------

# Load .env from parent folder
load_dotenv(dotenv_path=os.path.join(os.path.dirname(os.path.dirname(__file__)), ".env"))

HF_TOKEN = os.getenv("HF_TOKEN")
MODEL_ID = "meta-llama/Llama-3.1-8B-Instruct"

client = InferenceClient(model=MODEL_ID, token=HF_TOKEN)


# -------------------------
# Page config
# -------------------------
st.set_page_config(page_title="ğŸ¤– Chatbot Interface", page_icon="ğŸ¤–", layout="centered")

# Title
st.markdown("<h1 style='text-align:center; color: #4f008c'>ğŸ¤– Smart Assistant</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align:center; color:gray;'>Powered by Hugging Face API</p>", unsafe_allow_html=True)

# -------------------------
# Session state
# -------------------------
if "history" not in st.session_state:
    st.session_state.history = []

# Chat container
chat_container = st.container()

# User input at bottom
with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_input("ğŸ’¬ Type your message:", "")
    submitted = st.form_submit_button("Send")

if submitted and user_input:
    with st.spinner("ğŸ¤– Thinking..."):
        response = client.chat_completion(
            model=MODEL_ID,
            messages=[
                {"role": "system", "content": "You are a smart assistant"},
                {"role": "user", "content": user_input}
            ],
            max_tokens=200,
            temperature=0.7,
            top_p=0.9
        )
        reply = response.choices[0].message["content"]

    st.session_state.history.append(("you", user_input))
    st.session_state.history.append(("ğŸ¤– bot", reply))


# -------------------------
# Display chat history
# -------------------------
with chat_container:
    for sender, msg in st.session_state.history:
        if sender == "you":
            st.markdown(f"""
                <div style="background-color: #E6E6FA; padding:10px; border-radius:10px; margin:5px; text-align:right;">
                <b>ğŸ§‘ You:</b> {msg}
                </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
                <div style="background-color:#E6E6E6; padding:10px; border-radius:10px; margin:5px; text-align:left;">
                <b>ğŸ¤– Assistant:</b> {msg}
                </div>
            """, unsafe_allow_html=True)