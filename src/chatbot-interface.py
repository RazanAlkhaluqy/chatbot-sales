import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch


# Load model (CPU/GPU)
model_id = "meta-llama/Llama-3.1-8B"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")
llama_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0 if device=="cuda" else -1)

# Streamlit UI
st.title("ðŸ¤– LLaMA Chatbot")
user_input = st.text_input("Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:")

if st.button("Ø¥Ø±Ø³Ø§Ù„") and user_input:
    response = llama_pipeline(user_input, max_new_tokens=200)[0]["generated_text"]
    st.text_area("Ø§Ù„Ø±Ø¯:", value=response, height=200)
